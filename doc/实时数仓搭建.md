#### 1. Mysql配置binglog

##### 配置MySQL的binlog
```$xslt
[mysqld]
log-bin=mysql-bin # 开启 binlog
binlog-format=ROW # 选择 ROW 模式
server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复
```

##### 常见的binlog命令
```text
# 是否启用binlog日志
show variables like 'log_bin';
# 查看binlog类型
show global variables like 'binlog_format';
# 查看详细的日志配置信息
show global variables like '%log%';
# mysql数据存储目录
show variables like '%dir%';
# 查看binlog的目录
show global variables like "%log_bin%";
# 查看当前服务器使用的biglog文件及大小
show binary logs;
# 查看最新一个binlog日志文件名称和Position
show master status;
```

##### 授权
```sql
CREATE USER canal IDENTIFIED BY 'ceshiA@2020';  
GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%';
-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;
FLUSH PRIVILEGES;
```
#### 2. kafka、zookeeper安装（略）
```text
docker run -d --name zookeeper -p 2181:2181 -t wurstmeister/zookeeper
docker run  -d --name kafka -p 9092:9092 -e KAFKA_BROKER_ID=0 -e KAFKA_ZOOKEEPER_CONNECT=192.168.1.100:2181 -e KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://192.168.1.100:9092 -e KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092 -t wurstmeister/kafka
```
#### 3. 部署canal
下载解压：
```shell script
wget https://github.com/alibaba/canal/releases/download/canal-1.1.4/canal.deployer-1.1.4.tar.gz
tar -xzvf canal.deployer-1.1.4.tar.gz  -C /opt/canal
```
修改conf/example/instance.properties，修改内容如下：
```text
## mysql serverId
canal.instance.mysql.slaveId = 1234
#position info，需要改成自己的数据库信息
canal.instance.master.address = kms-1.apache.com:3306 
#username/password，需要改成自己的数据库信息
canal.instance.dbUsername = canal  
canal.instance.dbPassword = ceshiA@2020
# mq config，kafka topic名称
canal.mq.topic=test
```
修改conf/canal.properties，修改内容如下：
```text
# 配置zookeeper地址
canal.zkServers =kms-2:2181,kms-3:2181,kms-4:2181
# 可选项: tcp(默认), kafka, RocketMQ，
canal.serverMode = kafka
# 配置kafka地址
canal.mq.servers = kms-2:9092,kms-3:9092,kms-4:9092
```
启动canal
```shell script
sh bin/startup.sh
```
#### 4. 部署Canal Admin(可选)
下载解压：
```shell script
wget https://github.com/alibaba/canal/releases/download/canal-1.1.4/canal.admin-1.1.4.tar.gz
tar -xzvf canal.admin-1.1.4.tar.gz  -C /opt/canal-admin/
```
修改conf/application.yml，修改内容如下：
```yaml
server:
  port: 8089
spring:
  jackson:
    date-format: yyyy-MM-dd HH:mm:ss
    time-zone: GMT+8

spring.datasource:
  address: kms-1:3306
  database: canal_manager
  username: canal
  password: canal
  driver-class-name: com.mysql.jdbc.Driver
  url: jdbc:mysql://${spring.datasource.address}/${spring.datasource.database}?useUnicode=true&characterEncoding=UTF-8&useSSL=false
  hikari:
    maximum-pool-size: 30
    minimum-idle: 1

canal:
  adminUser: admin
  adminPasswd: 123456
```
初始化原数据库
```shell script
mysql -uroot -p123456 < /opt/canal-admin/conf/canal_manager.sql
```
启动canal-admin
```shell script
sh bin/startup.sh
```
登陆canal-admin就可以添加serve和instance

启动kafka控制台消费者测试
```shell script
bin/kafka-console-consumer.sh --bootstrap-server kms-2:9092,kms-3:9092,kms-4:9092  --topic test --from-beginning
```
此时MySQL数据表若有变化，会将row类型的log写进Kakfa，具体格式为JSON，比如insert操作：
```json
{
    "data":[
        {
            "id":"338",
            "city":"成都",
            "province":"四川省"
        }
    ],
    "database":"qfbap_ods",
    "es":1583394964000,
    "id":2,
    "isDdl":false,
    "mysqlType":{
        "id":"int(11)",
        "city":"varchar(256)",
        "province":"varchar(256)"
    },
    "old":null,
    "pkNames":[
        "id"
    ],
    "sql":"",
    "sqlType":{
        "id":4,
        "city":12,
        "province":12
    },
    "table":"code_city",
    "ts":1583394964361,
    "type":"INSERT"
}
```
JSON日志格式解释:
```text
data：最新的数据，为JSON数组，如果是插入则表示最新插入的数据，如果是更新，则表示更新后的最新数据，如果是删除，则表示被删除的数据
database：数据库名称
es：事件时间，13位的时间戳
id：事件操作的序列号，1,2,3…
isDdl：是否是DDL操作
mysqlType：字段类型
old：旧数据
pkNames：主键名称
sql：SQL语句
sqlType：是经过canal转换处理的，比如unsigned int会被转化为Long，unsigned long会被转换为BigDecimal
table：表名
ts：日志时间
type：操作类型，比如DELETE，UPDATE，INSERT
```

#### 5. flink 作业提交（略）

#### 6. flink 作业监控
##### flink配置
```text
cp opt/flink-metrics-prometheus-1.10.0.jar lib/
```
添加监控配置到flink-conf.yaml
```yaml
# Metric
metrics.reporter.promgateway.class: org.apache.flink.metrics.prometheus.PrometheusPushGatewayReporter
metrics.reporter.promgateway.host: xxx.xxx.xxx.xxx
metrics.reporter.promgateway.port: 9091
metrics.reporter.promgateway.jobName: flinkJob
metrics.reporter.promgateway.randomJobNameSuffix: true
metrics.reporter.promgateway.deleteOnShutdown: false
```
##### monitor部署
```shell script
tar -zxvf prometheus-2.17.2.linux-amd64.tar.gz -C /opt
tar -zxvf node_exporter-0.18.1.linux-amd64.tar.gz -C /opt
tar -zxvf pushgateway-1.2.0.linux-amd64.tar.gz -C /opt
tar -zxvf grafana-6.7.3.linux-amd64.tar.gz  -C /opt

nohup ./prometheus  >> ./log 2>&1 & echo $i > run.pid
nohup ./node_exporter  >> ./log 2>&1 & echo $i > run.pid
nohup ./pushgateway  >> ./log 2>&1 & echo $i > run.pid
nohup ./bin/grafana-server web  >> ./log 2>&1 & echo $i > run.pid
```
配置prometheus.yaml
```yaml
scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
        labels:
          instance: 'xxx.xxx.xxx.xxx'
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']
        labels:
          instance: 'xxx.xxx.xxx.xxx'
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['localhost:9091']
        labels:
          instance: 'xxx.xxx.xxx.xxx'
```
