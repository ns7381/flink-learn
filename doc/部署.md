#### standalone部署
##### 1. 配置flink-conf.yaml
```$xslt
# The heap size for the JobManager JVM
jobmanager.heap.mb: 1024

# The heap size for the TaskManager JVM
taskmanager.heap.mb: 1024

# The number of task slots that each TaskManager offers. Each slot runs one parallel pipeline.
taskmanager.numberOfTaskSlots: 4
# the managed memory size for each task manager.
taskmanager.managed.memory.size: 256
```
##### 2. 启动
```
./bin/start-cluster.sh 
```
##### 3. 提交作业
```
./bin/flink run examples/streaming/SocketWindowWordCount.jar --port 9000
```
##### 4. 停止集群
```cmd
./bin/stop-cluster.sh
```


#### Yarn（Session Cluster 模式）
##### 1. 配置环境变量HADOOP_CONF_DIR
##### 2. 启动yarn-session
```cmd
./bin/yarn-session.sh -n 2 -jm 4096 -tm 8192 -s 32
```
启动命令说明：
```$xslt
* -n,–container Number of TaskManagers
* -jm,–jobManagerMemory Memory for JobManager Container with optional unit (default: MB)
* -tm,–taskManagerMemory Memory per TaskManager Container with optional unit (default: MB)
* -qu,–queue Specify YARN queue.
* -s,–slots Number of slots per TaskManager
* -t,–ship Ship files in the specified directory (t for transfer)
```
##### 3. 提交作业
上一次创建Yarn session的集群信息保存在了'/tmp/.yarn-properties-${user}'文件中，默认提交Flink job不需指定对应 Yarn application 的信息
```cmd
./bin/flink run ./examples/batch/WordCount.jar -input hdfs://node1/tmp/jiazhou/test.txt -output hdfs://node1/tmp/output
```
如果同一用户在同一机器上再次创建一个 Yarn session，则这个文件会被覆盖掉，要在提交 Flink job 的命令中指明 Yarn 上的 Application ID，通过"-yid"参数传入
```cmd
./bin/flink run -yid application_1550579025929_62420 ./examples/batch/WordCount.jar -input hdfs://com:8020/txt -output hdfs://dcom:8020/cmd.txt
```
每次跑完任务不久，TaskManager 就被释放了，下次在提交任务的时候，TaskManager又会重新拉起来。
如果希望延长空闲 TaskManager 的超时时间，可以在 conf/flink-conf.yaml 文件中配置下面这个参数，单位是 milliseconds：
```text
slotmanager.taskmanager-timeout: 30000L         # deprecated, used in release-1.5
resourcemanager.taskmanager-timeout: 30000L
```

####  Yarn (Job Cluster)
##### 1. 配置环境变量HADOOP_CONF_DIR
##### 2. 提交作业

```$xslt
./bin/flink run -m yarn-cluster -p 4 -yjm 1024m -ytm 4096m examples/streaming/WordCount.jar --input hdfs:///test_dir/input_dir/story --output hdfs:///test_dir/output_dir/output
```
配置说明：
```text
* -yn,–yarncontainer Number of Task Managers
* -yqu,–yarnqueue Specify YARN queue.
* -ys,–yarnslots Number of slots per TaskManager
* -yqu,–yarnqueue Specify YARN queue.
```
#### Yarn HA配置
##### 1. 配置重启项
配置yarn集群"yarn-site.xml"
```text
<property>
  <name>yarn.resourcemanager.am.max-attempts</name>
  <value>10</value>
</property>
```
配置flink文件conf/flink-conf.yaml
```text
yarn.application-attempts: 10     # 1+ 9 retries
```
##### 2. 配置zookeeper
需要特别注意的是："high-availability.cluster-id"这个配置最好去掉，因为在Yarn模式下，cluster-id 如果不配置的话，会配置成 Yarn 上的 Application ID ，从而可以保证唯一性。
```$xslt
# 配置 high-availability mode
high-availability: zookeeper
# 配置 zookeeper quorum（hostname 和端口需要依据对应 zk 的实际配置）
high-availability.zookeeper.quorum: z05f02321.sqa.zth.tbsite.net:2181,z05f10215.sqa.zth.tbsite.net:2181
# （可选）设置 zookeeper 的 root 目录
high-availability.zookeeper.path.root: /test_dir/test_standalone2_root
# 删除这个配置
# high-availability.cluster-id: /test_dir/test_standalone2
# JobManager 的 meta 信息放在 dfs，在 zk 上主要会保存一个指向 dfs 路径的指针
high-availability.storageDir: hdfs:///test_dir/recovery2/
```

#### flink配置
```text
# Common
jobmanager.rpc.port: 6123
jobmanager.heap.mb: 4096m
taskmanager.heap.mb: 4096m
taskmanager.numberOfTaskSlots: 3
taskmanager.memory.preallocate: false
parallelism.default: 1

# Web Frontend
web.port: 8081

# Streaming state checkpointing
state.backend: rocksdb
state.backend.fs.checkpointdir: hdfs:///flink/checkpoints
state.savepoints.dir: hdfs:///flink/savepoints
 
# Advanced
taskmanager.tmp.dirs: /tmp

# High Availability
yarn.application-attempts: 10
high-availability: zookeeper
high-availability.storageDir: hdfs:///flink/recovery
high-availability.zookeeper.quorum: zkserver1:2181, zkserver2:2181, zkserver3:2181
high-availability.zookeeper.path.root: /flink
```
